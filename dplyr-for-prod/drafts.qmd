---
title: "dplyr is production"
format: html
---


I wont teach you how to select columns, filter rows, use cool dplyr verbs or talk about non-standard evaluation. I'm not really interested in that. Nor am I the best person to teach you that. But rather, I want to teach you how dplyr is ready to be used in production. I want to show you how you can write dplyr code that can scale. The same code can run on a few thousand rows, tens of thousands, and millions of rows all with very little change to your existing dplyr code. These lessons are to encourage you to think about your code differently. Not as R code, but as production-grade data engineering code. 

## Preface 

dplyr's impact on data sciece often goes under-recognized. It is known most often for its readable syntax and chainable expression. dplyr's undeniable ergonomics has led to impersonations and derivations across languauges: from the many python implementations such as [`siuba`](https://github.com/machow/siuba), [`dplython`](https://pythonhosted.org/dplython/), [`redframes`](https://github.com/maxhumber/redframes), [`dfply`](https://github.com/kieferk/dfply),  and now the great [Ibis](https://ibis-project.org/) project‚Äîwhich, as you'll see, shares more than just syntax‚Äîand I'm sure many more. One can also see the influence of dplyr's syntax in [Polars](https://docs.pola.rs/), the powerful rust based dataframe library with navtive bindings to python and community provided bindings to R. The [`TidierData.jl`](https://tidierorg.github.io/TidierData.jl/latest/) library is a 100% julia implementation that "stick[s] as closely to tidyverse syntax as possible."

They say that imitation is the highest form of flattery. But in this case, I think it also speaks to the impact made by dplyr and the broader tidyverse ecosystem. In the following lessons, we'll learn about the fundamental concepts that make dplyr so

## What does it mean to be "eager?"

Growing up my Dad always told me "if you're early, you're on time."
Eager evaluation wants to get it done right then and there. a "right now response time". Most functions that we use in R are eagerly evaluated. You run them, wait for them to come to completion, then have the results. It will do exaclty what you asked it to do‚Äîwhich is what we expect more often than not. 

But being quick on the draw is not always the most effective use of time. Sometimes it pays to be lazy. This is particularly true when working with large datasets.

A less common approach is lazy evaluation. 

## Being lazy pays off

<!-- shuold there be a way to click a word and have a popover of the definition? Like a glossary idea ? --> 
Lazy evaluation, or sometimes referred to as "call-by-need", delays execution until the very last moment. In R, lazy evaluation happens by default with function arguments. R's function arguments are _lazy_. Funtion arguments are not evaluated until they are actually needed. Which is powerful! But also may be confusing or the source of errors.

Take, this function for example:

```{r}
hello_world <- function(x) {
    "hello world"
}

hello_world()
```


It prints `"hello world"`, but did you notice something? We provided an argument `x`, that is never used! Since the argument `x` is never used, it will never be evaluated. That means we can commit crimes in the x argument but never get caught. 

```{r}
hello_world(stop("You shall not pass!!"))
```

We've tried to abort the function by calling `stop()` in the function argument! No dice. R is _lazy_ and since its lazy, its not going to stop since we never actually told it to stop. 

Let's try another example where we conditionally evaluate an argument.

```{r}
null_or_y <- function(x, y) {
    if (isTRUE(x)) {
        y 
    } else {
        NULL
    }
}
```

With this function we can pass a value of `TRUE` and it will return the value of `y`, if the value is `FALSE`, the function will return `NULL`.

If we pass in `TRUE`, the argument `y` is evaluated because we're returning the value. 

```{r}
null_or_y(TRUE, stop("stop right there! üëÆüèª‚Äç‚ôÇÔ∏è"))
```

However, if we provide `FALSE`...

```{r}
null_or_y(FALSE, stop("stop right there! üëÆüèª‚Äç‚ôÇÔ∏è"))
```

No error is emitted, and we get `NULL` returned back to us. 

## EXERCISE HERE:

TODO make exercise with lazy evaluation in a function

## dplyr is not just for the tibble

dplyr extends this concept of laziness to tables. 
- in the R community there has been this debate of base R vs dplyr and data.table vs dplyr!
- it's so divisive that I actually lost a job at DataCamp because of it‚Äîthat's a story to be shared over a beer
- data.table is another data.frame oriented data manipulation package
- it is, until recently with the publication of collapse, the fastest in memory data manipulation package in the R ecosystem
- there are a lot of reasons that data.table is faster than dplyr. Some of the high level reasons are that data.table is multithreaded and permits modify by reference as opposed to copy-on-modify
- to use data.table, one has to learn a really idiosyncratic "i, j, by" syntax (data.table uses super-powered version of base R's bracket indexing)

Using the nycflights13 dataset, we can count the number of flights by origin destinations, and arrange them. Using tradional dplyr our code might look like: 

```{r}
library(dplyr)
library(nycflights13)

flights |> 
    filter(carrier == "AA") |> 
    count(origin, dest) |> 
    arrange(origin, desc(dest))
```

The same code can be written fairly succinctly with data.table. However, if you don't know the syntax very well, like I don't, it can be tough to write.

```{r}
library(data.table)

# create a copy so that data.table can modify by reference
fl <- setDT(copy(flights))


fl[carrier == "AA", .N, by = .(origin, dest)][order(origin, -dest)]
```

Did it feel faster? I don't know if you could tell the difference from this one example, but I can assure you, it _is_ faster. Let's take a look: 

```{r}
bench::mark(
    dplyr = {
        flights |> 
            filter(carrier == "AA") |> 
            count(origin, dest) |> 
            arrange(origin, desc(dest)) 
    },
    dt = fl[carrier == "AA", .N, by = .(origin, dest)][order(origin, -dest)],
    check = FALSE
)

```

In this bench mark, data.table is just about 4.5 times _faster_ than dplyr! That's nothing to scoff at. Maybe even more significantly, data.table used far less memory! Sure, 10mb isn't too much now. But what about when we're working with 10 million rows in memory? You _will_ feel the difference then. 

To get this level of performance, you don't have to actually start writing data.table code. You can instead use `dtplyr`. 

```{r}
library(dtplyr)

fl_lazy <- lazy_dt(flights)

fl_lazy |>
    filter(carrier == "AA") |> 
    count(origin, dest) |> 
    arrange(origin, desc(dest)) 
```

The `lazy_dt()` function is the key here. It creates a `dtplyr_step` object which allows for this use of data.table and dplyr functions. 

```{r}
bench::mark(
    dtplyr = {
        fl_lazy |>
            filter(carrier == "AA") |> 
            count(origin, dest) |> 
            arrange(origin, desc(dest)) |> 
            collect()
    }, 
    dt = fl[carrier == "AA", .N, by = .(origin, dest)][order(origin, -dest)],
    check = FALSE
)
```

## Understanding the lazy table

- just created lazy table using dtplyr 
- lets spend some time understanding what is happening
- dplyr now acting as a "front-end" and data.table is acting as the "back-end" 
    - front-end refers to the thing you interact with
    - back-end is the thing that does the computation
- the dplyr code that we are writing is no longer eager.
- instead, the dplyr code is lazy it doesn't do anything.

assign the result of the dtplyr command and time how long this takes 

```{r}
system.time(
    res <- fl_lazy |>
        filter(carrier == "AA") |> 
        count(origin, dest) |> 
        arrange(origin, desc(dest))
)
```

Nothing was executed. Instead, dtplyr just created a set of instructions that we can investigate with `show_query()`

```{r}
show_query(res)
```

## The composable codex



```{r}
library(dplyr)
library(dtplyr)
library(nycflights13)

con <- dbplyr::src_memdb()
copy_to(con, nycflights13::flights, name = "flights")

flights <- tbl(con, "nycflights13::flights")

# nothing has happened yet...
avg_delay <- flights |> 
  filter(month == 1) |> 
  group_by(carrier) |> 
  summarise(
    min_delay = min(dep_delay),
    max_delay = max(dep_delay),
    avg_delay = mean(dep_delay)
  ) |> 
  arrange(-avg_delay)
```


- multiple objects 
- complex joins and queries by creating sub-queries 
- creating multiple objects has no cost because it's lazy

## Composable codex - modularity 


## Scaling up from traditional dplyr code 

## dtplyr 

- mutability / immutability
- or, copy on modify 
https://stackoverflow.com/questions/15759117/what-exactly-is-copy-on-modify-semantics-in-r-and-where-is-the-canonical-source



## Learning Objectives 

- conceptually, want to make it clear that writing dplyr is writing SQL
- learn about lazy computation

Outline: 

- What dplyr is
- It pays to be lazy
    - lazy vs eager computation
        - dplyr is eager
        - databases are lazy
- composable data systems 

- dplyr has been absolutely transformative for the data science ecosystem. 
- it defined a literate method of data analysis
- unprecedented amount of ergonomics built in
- but undoubtedly, dplyr's most transformative contribution to data science is underappreciated
- dplyr is a syntax that acts as a developer oriented API that is agnostic to the backend that is used
    - it allows for a distinction between the "front-end" and the "back-end"
- with dplyr, you can manipulate data.frames in memory, yah sure, that's just the beginning.
- Other supported dplyr backends:
    - data.table
    - duckdb
    - arrow
    - apache spark

"DBI separates the connectivity to the DBMS into a ‚Äúfront-end‚Äù and a ‚Äúback-end‚Äù."
    - https://dbi.r-dbi.org/ 
    - many R packages have been built upon it to provide database connectivity to R
    - some of the supported database back-ends
    - Postgres, MariaDB, BigQuery
    - odbc connections and more if using Posit professional drivers

Motivation: 
    - you can prototype code on simple csv files
    - the same dplyr code can then be used on bigger and bigger data just by changing the bac end
    - it also can be helpful in making it easier to avoid vendor lock in because the only vendor dependent code is going to be the connection object 

At NPD we trained SAS users on dplyr. Much of the SAS scripts involed first getting data from our databases and pre-processing it using proc sql. They would rewrite their code using dplyr and a csv file. Then we plugged it into spark with databricks with very little effort after that.


My recommendation for using dplyr: 

- start with basic dplyr for < 1m rows or when spending a few seconds computing isn't that big of a deal
- if you need to scale it a bit then I would recommend using a lazy data.table 

Restrictions: 

- you cannot mix R functions with SQL
- some stringr functions have dbplyr support 

To cover: 

- laziness in tables 
- dplyr remote table functions 
- dbplyr
    - https://dbplyr.tidyverse.org/articles/sql.html#what-happens-when-dbplyr-failsx
- creating DBI connections
- `{pool}` for managing multiple connections https://rstudio.github.io/pool/



Scheduling? Rscript, bash, cron

when using dbplyr try to work with tables
don't use %in% with sql its slow. Instead use a filtering join like  semi join


--------

dplyr is the inspiration for ibis in the python ecosystem
https://www.youtube.com/watch?v=XRxUeL3bQfQ&t=955s


arrow database connector
https://cloud.r-project.org/web/packages/adbi/index.html
