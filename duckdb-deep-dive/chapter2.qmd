---
title: DuckDB, `{DBI}`, `data.frame`s, and `{dplyr}`.
eval: false
---

# DuckDB as a database interface {.exercise}

The `{duckdb}` package implements methods for the R functions defined in the `{DBI}`—short for database interface—package. `{DBI}` can be used as a standalone SQL engine from within R. 

> [DBI] supports the following operations: 
> 
> -  connect/disconnect to the DBMS
> - create and execute statements in the DBMS
> - extract results/output from statements
> - error/exception handling
> - information (meta-data) from database objects
> - transaction management (optional)

DBI works by creating a connection object to a database. Each connection object requires a driver. The driver is what tells DBI how to connect to the database. In the case of DuckDB. You create a DuckDB database driver using the `duckdb()` function. 

```{r}
library(duckdb)
driver <- duckdb()
driver
```

Using the driver, you can create a connection to the database using DBI. The `driver` in this case points to an in-memory DuckDB database. The database connection object is created using `dbConnect()`. This is a `DBI` function that duckdb has implemented a method for. 

```{r}
con <- dbConnect(driver)
con
```

With the new connection object, you can begin to interact with the database itself.

You can write a `data.frame` as a table to your database with the `dbWriteTable()` function which has three arguments:

- `conn`: the connection object
- `name`: the name to assign the new table in your database
- `value`: the data.frame to write

Below, the `flights` dataset from the `nycflights13` package is written to the database. 

```{r}
dbWriteTable(con, "flights", nycflights13::flights)
```

To see the tables in your database us `dbListTables()` on your connection object. 

```{r}
dbListTables(con)
```

In the next exercise, you will learn how to query the database directly using DBI.

:::{.script-pane}

```{r, eval = FALSE}
library(duckdb)

# create a connection
con <- dbConnect(duckdb())

# write a table 
dbWriteTable(con, ___, ___)

# list tables in the connection
dbListTables(__)
```
:::

:::{.instructions}

- Create a DuckDB database instance using the default driver
- Write the `flights` data.frame from the `nycflights13` package to your database with the name `"flights"`
- List the tables in your database and make sure it shows the `flights` table

:::

# DBI basics {.exercise}

```{r include = FALSE}
# set up chunk
library(duckdb)
con <- dbConnect(duckdb())
dbWriteTable(con, "flights", nycflights13::flights)
```

The `{DBI}` package provides all of the utilities that you need to create and execute queries, manage tables, and anything else you might need. In this exercise we'll cover the basic utilites in `DBI`. In reality, you will likely not need these often but you should know that they exist. 

DBI allows you to send any SQL query directly to the database using `dbSendQuery()`. This function sends a query and creates a result. It does not, however, return the results as an R object.

```{r}
query <- dbSendQuery(con, "select * from flights")
query
```

With this we've only just executed the query. But to bring the results into R as a `data.frame` you need to _fetch_ the results using `dbFetch()`. `dbFetch()` is executed on the result object itself.

```{r}
res <- dbFetch(query)
head(res)
```

It is important to note that you can only fetch the results once! So ensure that you use the results or save them when you're done. If we want to try fetching them again, nothing is returned: 
```{r}
dbFetch(query)
```

This is good. We're making good use of our storage by not storing our data for longer than it is needed. After you fetch this query, the query result should be freed from memory. This is done with `dbClearResult()`

```{r}
dbClearResult(query)
```

Attempting to fetch the result again results in an error 

```{r, error = TRUE}
dbFetch(query)
```


This is a bit annoying though, huh? A lot to keep track of. If you want to send a query and fetch the results back into R, the process is simplified with `dbGetQuery()`.

Instead, we can count the number of flights per month and bring the results back into R in one fell swoop using `dbGetQuery()`. 

```{r}
dbGetQuery(con, "select month, count(*) as n from flights group by month")
```


Knowing the basics of DBI can be helpful if you want to integrate existing SQL code into your project or if there are tasks that you find easier to accomplish with SQL instead of writing dplyr code. You can do almost complete database administration using `DBI`. With familiarity of the basics of DBI, you should be able to navigate the documentation and figure out anything else you might need to do. 

Of course, this requires that you know SQL. And honestly? I'm not really a fan of SQL. It's not that I'm against it, it's just that it's yet another thing to remember. So why not use something we already know? This is why `dplyr` and `dbplyr` are so great! 


# Using dplyr with DuckDB

```{r include  = FALSE}
# setup chunk
library(duckdb)
con <- dbConnect(duckdb())
dbWriteTable(con, "flights", nycflights13::flights)
```

The main selling point of DuckDB as an R user is its integration with dplyr. As discussed in the course [Zero to Prod with dplyr](TODO), dplyr is not only data manipulation library. Instead, it has become a syntax for data analysis. dplyr provides a standard syntax for data manipulation. It is a frontend that can be applied to many backends. 

In our case, we will use dplyr as a frontend to a DuckDB backend. 

## Table connections 

In our DuckDB database, we created a table, `flights`. 

```{r}
dbListTables(con)
```

We saw how it can be queried using DBI and standard SQL. But instead, it is much easier to work with these tables using dplyr and the syntax that we know and love. 

Similar to the database connection object we created using `duckdb()` we need to create a reference to a table. With dplyr, this is done using the `tbl()` function. We need to provide it with two arguments: 

- `con`: the connection object
- `...`: the name (quoted) of the table that we want to access

To create a reference to the flights table in the DuckDB database use `tbl(con, "flights")`.

```{r}
library(dplyr)

# create a connection to the flights database table
fl <- tbl(con, "flights")
fl
```

With this, you now have access to a lazy table that references the table in the database. 

# Lazy mother ducks

```{r include  = FALSE}
# setup chunk
library(duckdb)
con <- dbConnect(duckdb())
dbWriteTable(con, "flights", nycflights13::flights)

library(dplyr)
fl <- tbl(con, "flights")

early_deps <- fl |> 
    filter(dep_delay < 0) |> 
    group_by(carrier) |> 
    summarise(
        avg_dep_delay = mean(dep_delay, na.rm = TRUE),
        avg_arr_delay = mean(arr_delay, na.rm = TRUE)
    ) |> 
    arrange(avg_arr_delay)
```

The table connections that are created by dplyr when working with any database connection are **lazy** in nature. 

```{r}
class(fl)
```

Notice the `tbl_lazy` subclass. This is really important. It means that everything we do on the table will not actually execute unless we demand it of the table. 

For example, say you were interested in all flights that departed _early_ and wanted to know how much earlier those flights arrived on average based on the airline carrier. You might write some dplyr code like this: 

```{r}
early_deps <- fl |> 
    filter(dep_delay < 0) |> 
    group_by(carrier) |> 
    summarise(
        avg_dep_delay = mean(dep_delay, na.rm = TRUE),
        avg_arr_delay = mean(arr_delay, na.rm = TRUE)
    ) |> 
    arrange(avg_arr_delay)

early_deps
```

Notice that this is still a lazy table and _not_ a tibble or a dataframe. Instead, it is just a collection of SQL instructions that will be sent to the database. To get insight into what is being sent to the database, use `show_query()`

```{r}
show_query(early_deps)
```

# Collecting query results 

```{r include  = FALSE}
# setup chunk
library(duckdb)
con <- dbConnect(duckdb())
dbWriteTable(con, "flights", nycflights13::flights)

library(dplyr)
fl <- tbl(con, "flights")
early_deps <- fl |> 
    filter(dep_delay < 0) |> 
    group_by(carrier) |> 
    summarise(
        avg_dep_delay = mean(dep_delay, na.rm = TRUE),
        avg_arr_delay = mean(arr_delay, na.rm = TRUE)
    ) |> 
    arrange(avg_arr_delay)
```

In order to use the results of this query with most other R packages, it wil need to be brought into memory as an object that R natively knows how to work with. In the case of SQL tables and queries, this is a data.frame. 

To finally execute the query, you must run `dplyr::collect()`. This forces the database to run the query and fetch the results into memory. Remember the three step process of `dbSendQuery()`, `dbFetch()`, and `dbClearResult()`? dplyr will handle _all_ of this overhead for you! 

```{r}
early_df <- collect(early_deps)
early_df
```

With this, you can now work with the data in memory as you would want to for example to use with something like `ggplot2`, `plotly`, `gt` or else.

I cannot express this enough: 

Bring into memory **as little data as possible**.

The purpose of using lazy tables and databases is that it offloads the computation from R into the databases that are expertly designed for these processes. If you collect results early _and then_ filter and summarise your results, you will have a bad time and will be no better off than having read everything into memory to begin with. So, heed my advice! 

`collect()` as late as possible!

# Combining lazy tables 


```{r include  = FALSE}
# setup chunk
library(duckdb)
con <- dbConnect(duckdb())
dbWriteTable(con, "flights", nycflights13::flights)
early_deps <- fl |> 
    filter(dep_delay < 0) |> 
    group_by(carrier) |> 
    summarise(
        avg_dep_delay = mean(dep_delay, na.rm = TRUE),
        avg_arr_delay = mean(arr_delay, na.rm = TRUE)
    ) |> 
    arrange(avg_arr_delay)
```


We just saw how dplyr will lazily create SQL for you using the `show_query()` function. In the previous exercise, we stored the average departure and arrival delays for early flights. What if we wanted to find the opposite? Where we find the departure and arrival delay for _late_ flights? This would look very similar

```{r}
late_deps <- early_deps <- fl |> 
    filter(dep_delay > 0) |> 
    group_by(carrier) |> 
    summarise(
        avg_dep_delay = mean(dep_delay, na.rm = TRUE),
        avg_arr_delay = mean(arr_delay, na.rm = TRUE)
    ) 

show_query(late_deps)
```

In order to combine these results, we can join on the `carrier` name. We can do this with these lazy tables! And performing operations on them together will create more SQL optimized SQL code on your behalf. 

```{r}
all_dep_delays <- left_join(
    early_deps, 
    late_deps,
    by = "carrier",
    suffix = c("_early", "_late")
)

show_query(all_dep_delays)
```


These lazy tables can be combined in any of the ways in which you would normally use a tibble. Lean on them to simplify your code and your workflows and only collect the results when you need to have a data.frame. 

:::{.callout-tip}
If you find yourself using dplyr verbs after calling `collect()` there is a good chance that you collected too early and could have offloaded your work to DuckDB.

The more data you collect, the more data you have to store in memory and the slower your R code will become. 
:::


All of what we have covered has been in the context of objects that are already in R's memory. However, the key utility of DuckDB is its ability to work lazily over files without reading them into memory. 

In the next chapter we will go over data import using DuckDB. 


<!-- 
https://solutions.posit.co/connections/db/r-packages/dbi/ -->