---
title: Importing Data
--- 

# Getting data in

In this chapter you will learn how to read

- [`csv`](https://en.wikipedia.org/wiki/Comma-separated_values) files ([doc](https://duckdb.org/docs/data/csv/overview))
- [`parquet`](https://parquet.apache.org/) files ([doc](https://duckdb.org/docs/data/parquet/overview))
- many files at once ([doc](https://duckdb.org/docs/data/multiple_files/overview))
- parititioned datasets ([doc](https://duckdb.org/docs/data/partitioning/hive_partitioning))


------------


A database isn't much use without any data. Traditionally, databases load data using an `INSERT INTO` statement. Fortunately, DuckDB has made it much easier to import data into your database. 

Out of the box, DuckDB supports the most common analytical data file types: 

- CSV
- Parquet
- JSON

Additionally, DuckDB supports other strategies commonly employed when storing larger data on disc. You may have your data stored as many CSVs in a directory: 

```
orders
├── file001.csv
├── file002.csv
├── file003.csv
├── file004.csv
└── file005.csv
```

Or, your data may be stored using the Hive partitioning strategy. Below is the example [used in the DuckDB documentation](https://duckdb.org/docs/data/partitioning/hive_partitioning.html)

```
orders
├── year=2021
│    ├── month=1
│    │   ├── file1.parquet
│    │   └── file2.parquet
│    └── month=2
│        └── file3.parquet
└── year=2022
     ├── month=11
     │   ├── file4.parquet
     │   └── file5.parquet
     └── month=12
         └── file6.parquet
```

Next, we'll focus on importing individual files into your database.

# Reading CSV files

While we in the data science community like to talk about the importance and benefit of using columnar data storage formats like parquet, we cannot deny that csv files are still the most common format. We will start there. 

Like the tidyverse's [`{readr}`](https://readr.tidyverse.org/) has a function `read_csv()`, so too does DuckDB have a `read_csv()` function! `read_csv()` is a SQL function that is used to create a table reference. The DuckDB CSV reader is ridiculously performant and robust. 

> "All clean CSVs are alike; each messy CSV is messy in its own way." - Leo Quackstoy

 The developers at [MotherDuck](https://motherduck.com/) have put a lot of effort into being able to handle the many edge cases that are encountered when working with CSV files. They have a [**CSV Sniffer**](https://duckdb.org/2023/10/27/csv-sniffer) which automatically [detects and handles errors](https://duckdb.org/docs/data/csv/reading_faulty_csv_files) and problems. 

## Read using SQL 

To illustrate how to use the CSV reader, we should start in SQL. First we create an in-memory database.

```{r}
library(dplyr)
library(duckdb)

# create the connection
con <- dbConnect(duckdb())
```

Next, we can count the number of records in a CSV file. We will use the `data/COBRA-2022.csv` file. It contains data from Atlanta PD's [Open Data Portal](https://opendata.atlantapd.org/).

The query is very simple, it counts the number of rows. However, rather than pointing to a table directly like one would do with a `from` statement, the `read_csv()` function is used with a path to the file that needs to be read.

```{r}
# Create a query
query <- "
select count(*) as n
from read_csv('data/COBRA-2022.csv')
"

dbGetQuery(con, query)
```

For about 90% of use cases, you will not need to modify any of the parameters in the `read_csv()` function. But, in the event that you do you can specify them in line. Some of the important parameters that you might want to know that exist are: 

- `columns`: key-value pairs of the column name to type to parse it as
- `delim`: value delimeter. For example set `delim = "|"` when using pipe separated values
- `names`: the column names to use e.g. `["id", "name", "value"]`

# Creating lazy tables from CSV

You now know how to read a csv file using DuckDB and SQL directly. The goal isn't for you to learn SQL, but to appreciate and have a vague understanding of what is happening behind the scenes. The SQL `read_csv()` function is abstracted away from you, generally, by the `duckdb::tbl_file()` function. 

`tbl_file()` will create a lazy table from a DuckDB DBI connection and path to a file. 

```{r}
crimes <- tbl_file(con, "data/COBRA-2022.csv")
crimes
```

Alternatively, the `tbl()` function from `dplyr` can be used. This function requires the name of the table that you want to use. However, the CSV does not yet exist in the database. So when using the `tbl()` function, you must use the DuckDB `read_csv()` function inside of it. 

The equivalent code is: 

```{r}
tbl(con, 'read_csv("data/COBRA-2022.csv")')
```

## When should I use `tbl()` over `tbl_file()`?

Wow, great question! I'm so glad you asked that. 

You should use `tbl_file()` in almost all circumstances. When `tbl_file()` fails you, it is likely because you need to pass additional arguments to DuckDB's CSV sniffer directly. And to do that, you will need to use the `dplyr::tbl()` function instead. 


# TL;DR: Parquet

Another increasingly common file type is Parquet. Before diving into how to read a parquet file, it would behoove you to know _why_ parquet is so great. Like Apache Arrow & DuckDB, Parquet is columnar.

**The difference between Arrow and Parquet** is that  

:::{.text-3xl .text-center}
Apache Arrow is an _in memory_ representation whereas parquet is an _on disc_ format.
:::

Parquet is how you _store_ data whereas Arrow is how you hold data in memory after you read it! We won't go into detail into how and why parquet is so wonderful but at a highlevel there are some key concepts that you need to understand. 

## Why Parquet is so efficient 

First, the data is **stored by column**. This means that when you read from a parquet file you can read only the columns you need. Selecting only the columns you need from a parquet file is called [**projection**](https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/#projection-pushdown).

Next, **each column itself is partitioned into chunks**. These column chunks can be as large or small as you'd like. Finding a happy medium is important. Each **column chunk has associated metadata**. For example, if you are storing a numeric column, the metadata of the column chunk will contain the minimum and maximum values for that chunk. Using this metadata you can filter out rows that are not needed in your analysis. This is called [**predicate pushdown**](https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/#predicate-pushdown).

 For example, the chunk has a minimum value of 10 and a maximum value of 200 but your query fitlers to values less than 0, you know that you can safely skip reading an entire group of rows. Reading in as little data as possible is a key pattern of working with big data and parquet helps you do this efficiently. 

Lastly, and just as important, parquet utilizes a number of compression algorithms to effectively reduce the overall size of the files. Compression allows the file size to be smaller than the amount of data stored in the file. Read the [DuckDB blog post](https://duckdb.org/2024/03/26/42-parquet-a-zip-bomb-for-the-big-data-age.html) about how a 42kb file can store more than 4 petabytes of data. 

There are, as always, a number of considerations to make in deciding if parquet files are right for you. But as a data scientists, it is important for you to know what they are and how they work so _you_ can make that informed decision!

## To recap

Parquet

- stores data by columns allowing for **projection**
- each column is chunked into `n` values,
- column chunks have associated metadata allowing for **predicate pushdown**,
- applies compression algorithms to ensure smaller file sizes.


# Reading Parquet 

DuckDB makes reading parquet files really easy. In fact, it is even easier than reading CSV files and even more flexible. When used with SQL and DuckDB, you can provide the path of the parquet directly in the `FROM` clause. 

```{r}
dbGetQuery(con, "select count(*) as n from 'data/yellowcab/week-1.parquet'")
```

Just like using `tbl_file()` with the path of a csv file, the same can be done for parquet. 

```{r}
tbl_file(con, "data/yellowcab/week-1.parquet")
```

However, it is common to have your dataset partitioned over multiple files. You can create a single table from multiple files in a few ways. 

First, you can use DuckDB's `read_parquet()` function which allows you to specify an array of files to be read together as a single table like so: `read_parquet(['file1.parquet', 'file2.parquet'])`

```{r}
tbl(
  con,
  "read_parquet(['data/yellowcab/week-1.parquet', 'data/yellowcab/week-2.parquet'])"
)
```

More often than not, though, a whole directory will be dedicated to storing a single table in many parquet files. This can be specified using glob syntax. 

In the case of the `yellowcab/` directory there are 5 parquet files. These can all be read using the path `"data/yellowcab/*.parquet"`.

```{r}
tbl(con, "data/yellowcab/*.parquet")
```

:::{.callout-tip}
This same syntax can be used for csv files!
:::

# Hive partitioning


- autodetected by default when the folders are named with the `key=value`

```
orders
└── year=2022
     └── month=11
         ├── file4.parquet
         └── file5.parquet
```

- the approach is generally the same
- its useful to understand hive partitioning

```{r}
tbl(con, "taxi-data-2019-partitioned/**/*.parquet") |>
  group_by(vendor_name) |>
  count()
tbl(con, "read_parquet('taxi-data-2019-partitioned/**/*.parquet', hive_partitioning = true)")
```


https://duckdb.org/docs/data/partitioning/hive_partitioning