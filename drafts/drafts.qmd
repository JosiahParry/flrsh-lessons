---
title: "dplyr is production"
format: html
description: > 
  I wont teach you how to select columns, filter rows, use cool dplyr verbs or talk about non-standard evaluation. I'm not really interested in that. Nor am I the best person to teach you that. But rather, I want to teach you how dplyr is ready to be used in production. I want to show you how you can write dplyr code that can scale. The same code can run on a few thousand rows, tens of thousands, and millions of rows all with very little change to your existing dplyr code. These lessons are to encourage you to think about your code differently. Not as R code, but as production-grade data engineering code.
prereqs: 
  - dplyr
  - writing functions
---

```{r, include = FALSE}
knitr::opts_chunk$set(error = TRUE, message = FALSE)
```


## Outline:


- eager evaluation
- lazy evaluation
- lazy tables 


## Preface

dplyr's impact on data science often goes under-recognized. It is known most often for its readable syntax and chainable expression. dplyr's undeniable ergonomics has led to impersonations and derivations across languages: from the many python implementations such as [`siuba`](https://github.com/machow/siuba), [`dplython`](https://pythonhosted.org/dplython/), [`redframes`](https://github.com/maxhumber/redframes), [`dfply`](https://github.com/kieferk/dfply),  and now the great [Ibis](https://ibis-project.org/) project—which, as you'll see, shares more than just syntax—and I'm sure many more. One can also see the influence of dplyr's syntax in [Polars](https://docs.pola.rs/), the powerful rust based dataframe library with navtive bindings to python and community provided bindings to R. The [`TidierData.jl`](https://tidierorg.github.io/TidierData.jl/latest/) library is a 100% Julia implementation that "stick[s] as closely to tidyverse syntax as possible."

They say that imitation is the highest form of flattery. But in this case, I think it also speaks to the impact made by dplyr and the broader tidyverse ecosystem. 

## R and evaluation

Before we dive into the nuts and bolts of how what makes dplyr so powerful in production code, we need to first have a good understanding of the two primary types of evaluation strategies in R. We need to first be able to understand the distinction between eager and lazy evaluation. 


## What does it mean to be "eager?"

dplyr, and the vast majority of code that we run in R is eagerly evaluated. 

```{r}
library(dplyr)

skin_colors <- starwars |>
  count(skin_color) |>
  arrange(-n)
```

When we execute this expression, it runs right to completion no questions asked. Eager evaluation wants to get the job done right then and there. Most functions that we use in R are eagerly evaluated. You run them, wait for them to come to completion, then have the results. 

But being quick on the draw is not always the most effective use of time. Sometimes it pays to be lazy. This is particularly true when working with large datasets. A less common approach is _**lazy evaluation**_.

## Being lazy pays off

<!-- shuold there be a way to click a word and have a popover of the definition? Like a glossary idea ? -->
Lazy evaluation, or sometimes referred to as "call-by-need", delays execution until the very last moment. In R, function arguments are lazily evaluated. That means that the value of them are not computed until they are actually needed. This is very powerful! It let's us write code like this: 

```{r}
lazyy <- function(x, y = x + 2) {
  x + y
}

lazyy(2)
```

The value of `y` is set to the value of `x` plus `2` but we don't actually know the value of `x` until it's provided as an argument. For a bit clearer example, take, this function.

```{r}
hello_world <- function(x) {
  "hello world"
}

hello_world()
```


It prints `"hello world"`, but did you notice something? We provided an argument `x`, that is never used! Since the argument `x` is never used, and arguments are lazily evaluated, it will never be executed. That means we can commit crimes in the x argument but never get caught.

```{r}
hello_world(stop("You shall not pass!!"))
```

We've tried to abort the function by calling `stop()` in the function argument. No dice.

Let's try another example where we conditionally evaluate an argument.

```{r}
null_or_y <- function(x, y) {
  if (isTRUE(x)) {
    y
  } else {
    NULL
  }
}
```

With this function we can pass a value of `TRUE` and it will return the value of `y`, if the value is `FALSE`, then function will return `NULL`.


When `x = TRUE`, the argument `y` is evaluated. 

```{r error =TRUE}
null_or_y(TRUE, stop("stop right there! 👮🏻‍♂️"))
```

However, if we provide `FALSE`...

```{r error = TRUE}
null_or_y(FALSE, stop("stop right there! 👮🏻‍♂️"))
```

No error is emitted, and we get `NULL` returned back to us.

## Create your own lazy function

TODO make exercise with lazy evaluation in a function

## Making dplyr lazy

dplyr extends this concept of this laziness to data.frames and tables more generally. We'll explore the concept of lazy tables through the package [`{dtplyr}`](https://dtplyr.tidyverse.org/). 

In the R community there has been this debate of base R versus dplyr versus data.table for years now. It is _very_ divisive. It's so divisive that I actually lost a job at DataCamp because of it. Though that's a story to be shared over a beer.

[`data.table`](https://rdatatable.gitlab.io/data.table/) is another data.frame oriented data manipulation package. It is indisputably, until recently with the publication of [`collapse`](https://sebkrantz.github.io/collapse/), the fastest in memory data.frame manipulation package in the R ecosystem. However, to use data.table, one has to learn a fairly idiosyncratic "i, j, by" syntax (data.table uses super-powered version of base R's bracket indexing). Learning a new syntax can be a deterrent which is why I think data.table is not as widely adopted as perhaps ought to be. 

As a motivating example, let's adapt the example from the [Introduction to data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) vignette. 

Using the [`nycflights13`](https://cran.r-project.org/web/packages/nycflights13/index.html) dataset, we can count the number of American Airlines flights by origin, destinations, and arrange them. Using traditional dplyr our code might look like this:

```{r}
library(dplyr)
library(nycflights13)

flights |>
  filter(carrier == "AA") |>
  count(origin, dest) |>
  arrange(origin, desc(dest))
```

The same code can be written fairly succinctly with data.table. However, if you don't know the syntax very well, like I don't, it can be quite terse.

```{r}
library(data.table)

# create a copy so that data.table can modify by reference
fl <- setDT(copy(flights))

fl[carrier == "AA", .N, by = .(origin, dest)][order(origin, -dest)]
```

Did it feel faster? I don't know if you could tell the difference from this one example, but I can assure you, it _is_ faster. Let's take a look by creating a [`bench::mark()`](https://bench.r-lib.org/).

:::{.callout-note} 
`bench::mark()` works by providing named expressions that are each evaluated and time multiple times. 
:::
```{r}
bench::mark(
  dplyr = {
    flights |>
      filter(carrier == "AA") |>
      count(origin, dest) |>
      arrange(origin, desc(dest))
  },
  dt = fl[carrier == "AA", .N, by = .(origin, dest)][order(origin, -dest)],
  check = FALSE
)
```

In this bench mark, data.table is just about 4.5 times _faster_ than dplyr! That's nothing to scoff at. Maybe even more significantly, data.table used far less memory! Sure, 10mb isn't too much now. But what about when we're working with 10 million rows in memory? You _will_ feel the difference then.

To get this level of performance, you don't have to actually start writing data.table code. You can instead use `dtplyr`.

```{r}
library(dtplyr)

fl_lazy <- lazy_dt(flights)

fl_lazy |>
  filter(carrier == "AA") |>
  count(origin, dest) |>
  arrange(origin, desc(dest))
```

The `lazy_dt()` function is the key here. It creates a `dtplyr_step` object which allows us to make use of data.table via dplyr functions. The print method of the results are very informative. We'll dive into this in the next section. But before then, Let's make another bench mark to compare the results. Don't worry about all of the code in there, we'll go over it shortly. 

```{r}
bench::mark(
  dtplyr = {
    fl_lazy |>
      filter(carrier == "AA") |>
      count(origin, dest) |>
      arrange(origin, desc(dest)) |>
      collect()
  },
  dt = fl[carrier == "AA", .N, by = .(origin, dest)][order(origin, -dest)],
  check = FALSE
)
```


## Understanding the lazy table

In the previous section we created what is referred to as a lazy table. Let's spend some time understanding what we did. The function `lazy_dt()` converted our tibble into a _lazy_ data.table. 

The [manual page](https://dtplyr.tidyverse.org/reference/lazy_dt.html) for `lazy_dt()` says

> "A lazy data.table captures the intent of dplyr verbs, only actually performing computation when requested..."

This is similar to the concept of lazy function argument evaluation. But instead of delaying evaluation, dtplyr is building up an expression that will eventually be evaluated. 

```{r}
res <-
  fl_lazy |>
  filter(carrier == "AA") |>
  count(origin, dest) |>
  arrange(origin, desc(dest))

res
```

Above the data preview we can see the `Call` that has been created. dtplyr is _translating_ our dplyr code into data.table code. In essence, our dplyr code is acting as a **front-end** while data.table is acting the **back-end**. In this case, the front-end is the user-facing interface—in this case the user is the developer—and the backend is where the computation is occurring. We can see the translation that dtplyr has built up by using `dplyr::show_query()`.

```{r}
show_query(res)
```

dtplyr is essentially acting is a translator between a common syntax in dplyr and an execution engine in data.table. dtplyr is being lazy in that it is not running any code but instead it is building up a set of instructions that will be executed. 

To make dtplyr actually execute the code that it has built up, we need to request the results. This is done by using `dplyr::collect()`.

:::{.callout-note}
Note that you can also use `as.data.table()`, `as_tibble()`, `as.data.frame()` to bring the results into a data.frame.
:::

```{r}
collect(res)
```


## The composable codex

- composable data ecosystem
- modularity 
https://ibis-project.org/concepts/composable-ecosystem.html


```{r}
library(dplyr)
library(dtplyr)
library(nycflights13)

con <- dbplyr::src_memdb()
copy_to(con, nycflights13::flights, name = "flights")

flights <- tbl(con, "nycflights13::flights")

# nothing has happened yet...
avg_delay <- flights |>
  filter(month == 1) |>
  group_by(carrier) |>
  summarise(
    min_delay = min(dep_delay),
    max_delay = max(dep_delay),
    avg_delay = mean(dep_delay)
  ) |>
  arrange(-avg_delay)
```


- multiple objects
- complex joins and queries by creating sub-queries
- creating multiple objects has no cost because it's lazy

## Composable codex - modularity


## Scaling up from traditional dplyr code

## dtplyr

- mutability / immutability
- or, copy on modify
https://stackoverflow.com/questions/15759117/what-exactly-is-copy-on-modify-semantics-in-r-and-where-is-the-canonical-source



## Learning Objectives

- conceptually, want to make it clear that writing dplyr is writing SQL
- learn about lazy computation

Outline:

- What dplyr is
- It pays to be lazy
    - lazy vs eager computation
        - dplyr is eager
        - databases are lazy
- composable data systems


- dplyr is a syntax that acts as a developer oriented API that is agnostic to the backend that is used
    - it allows for a distinction between the "front-end" and the "back-end"
- with dplyr, you can manipulate data.frames in memory, yah sure, that's just the beginning.
- Other supported dplyr backends:
    - data.table
    - duckdb
    - arrow
    - apache spark

"DBI separates the connectivity to the DBMS into a “front-end” and a “back-end”."
    - https://dbi.r-dbi.org/
    - many R packages have been built upon it to provide database connectivity to R
    - some of the supported database back-ends
    - Postgres, MariaDB, BigQuery
    - odbc connections and more if using Posit professional drivers

Motivation:
    - you can prototype code on simple csv files
    - the same dplyr code can then be used on bigger and bigger data just by changing the bac end
    - it also can be helpful in making it easier to avoid vendor lock in because the only vendor dependent code is going to be the connection object

At NPD we trained SAS users on dplyr. Much of the SAS scripts involed first getting data from our databases and pre-processing it using proc sql. They would rewrite their code using dplyr and a csv file. Then we plugged it into spark with databricks with very little effort after that.


My recommendation for using dplyr:

- start with basic dplyr for < 1m rows or when spending a few seconds computing isn't that big of a deal
- if you need to scale it a bit then I would recommend using a lazy data.table
- at the end of the day, we want to bring as little into R's memory as we possibly.

Restrictions:

- you cannot mix R functions with SQL
- some stringr functions have dbplyr support

To cover:

- laziness in tables
- dplyr remote table functions
- dbplyr
    - https://dbplyr.tidyverse.org/articles/sql.html#what-happens-when-dbplyr-failsx
- creating DBI connections
- `{pool}` for managing multiple connections https://rstudio.github.io/pool/



Scheduling? Rscript, bash, cron

when using dbplyr try to work with tables
don't use %in% with sql its slow. Instead use a filtering join like  semi join


--------
https://www.youtube.com/watch?v=QC7WktPt7ow

"[dbplyr] takes R from just a simple tool to an incredibly powerful analysis platform"

dbplyr is for RDBMS relational database management systems




dplyr is the inspiration for ibis in the python ecosystem
https://www.youtube.com/watch?v=XRxUeL3bQfQ&t=955s


arrow database connector
https://cloud.r-project.org/web/packages/adbi/index.html
